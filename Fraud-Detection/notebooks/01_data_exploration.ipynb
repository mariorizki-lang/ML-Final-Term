{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "iJhkfjDjWWbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9RhL5Cb5WMUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gc\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNT GOOGLE DRIVE**"
      ],
      "metadata": {
        "id": "jRTi0-wxW209"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "project_folder = '/content/drive/MyDrive/FraudDetection'  # Sesuaikan dengan folder kamu\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print(f\"Working Directory: {os.getcwd()}\")\n",
        "print(\"\\nâœ… Google Drive mounted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv3hTXDBW5DU",
        "outputId": "05e4e5a2-9390-4be5-f224-ea3b752e1acb"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Working Directory: /content/drive/MyDrive/FraudDetection\n",
            "\n",
            "âœ… Google Drive mounted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD DATA & COLUMN INFO**"
      ],
      "metadata": {
        "id": "Cv2NnEavXIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load training data\n",
        "train_df = pd.read_csv('/content/drive/MyDrive/datasets/train_transaction.csv')\n",
        "print(f\"Original Dataset Shape: {train_df.shape}\")\n",
        "print(f\"Memory Usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Load column info from Notebook 1\n",
        "with open('results/01_column_info.pkl', 'rb') as f:\n",
        "    column_info = pickle.load(f)\n",
        "\n",
        "numeric_cols = column_info['numeric_columns']\n",
        "categorical_cols = column_info['categorical_columns']\n",
        "\n",
        "print(f\"\\nNumeric Columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical Columns: {len(categorical_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxzLcEq6XJgM",
        "outputId": "5b9666f9-8abc-4733-941d-972da8c12517"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LOADING DATA\n",
            "============================================================\n",
            "Original Dataset Shape: (590540, 394)\n",
            "Memory Usage: 2062.07 MB\n",
            "\n",
            "Numeric Columns: 379\n",
            "Categorical Columns: 14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**REMOVE DUPLICATES**"
      ],
      "metadata": {
        "id": "CTfNbOc0Xcae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 1: REMOVING DUPLICATES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "initial_rows = len(train_df)\n",
        "train_df = train_df.drop_duplicates()\n",
        "removed_duplicates = initial_rows - len(train_df)\n",
        "\n",
        "print(f\"Removed {removed_duplicates} duplicate rows\")\n",
        "print(f\"Remaining rows: {len(train_df):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1CPCcR6XddN",
        "outputId": "c48bcfcf-8333-49a2-947a-547a93595699"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 1: REMOVING DUPLICATES\n",
            "============================================================\n",
            "Removed 0 duplicate rows\n",
            "Remaining rows: 590,540\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DROP COLUMNS WITH >50% MISSING VALUES**"
      ],
      "metadata": {
        "id": "6jKXVtd-XkAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 2: DROPPING COLUMNS WITH >50% MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate missing percentage\n",
        "missing_pct = (train_df.isnull().sum() / len(train_df)) * 100\n",
        "cols_to_drop = missing_pct[missing_pct > 50].index.tolist()\n",
        "\n",
        "print(f\"Columns to drop ({len(cols_to_drop)}):\")\n",
        "if len(cols_to_drop) > 0:\n",
        "    for col in cols_to_drop[:20]:  # Show first 20\n",
        "        print(f\"  - {col}: {missing_pct[col]:.2f}% missing\")\n",
        "    if len(cols_to_drop) > 20:\n",
        "        print(f\"  ... and {len(cols_to_drop) - 20} more\")\n",
        "\n",
        "    train_df = train_df.drop(columns=cols_to_drop)\n",
        "\n",
        "    # Update column lists\n",
        "    numeric_cols = [col for col in numeric_cols if col not in cols_to_drop]\n",
        "    categorical_cols = [col for col in categorical_cols if col not in cols_to_drop]\n",
        "\n",
        "    print(f\"\\nâœ“ Dropped {len(cols_to_drop)} columns\")\n",
        "else:\n",
        "    print(\"âœ“ No columns with >50% missing values\")\n",
        "\n",
        "print(f\"Current shape: {train_df.shape}\")\n",
        "\n",
        "# Save dropped columns list\n",
        "with open('results/02_dropped_columns.pkl', 'wb') as f:\n",
        "    pickle.dump(cols_to_drop, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiukuKyGXlkE",
        "outputId": "7cf028c6-1581-4199-f7ce-f6568079c1ed"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 2: DROPPING COLUMNS WITH >50% MISSING VALUES\n",
            "============================================================\n",
            "Columns to drop (174):\n",
            "  - dist1: 59.65% missing\n",
            "  - dist2: 93.63% missing\n",
            "  - R_emaildomain: 76.75% missing\n",
            "  - D5: 52.47% missing\n",
            "  - D6: 87.61% missing\n",
            "  - D7: 93.41% missing\n",
            "  - D8: 87.31% missing\n",
            "  - D9: 87.31% missing\n",
            "  - D12: 89.04% missing\n",
            "  - D13: 89.51% missing\n",
            "  - D14: 89.47% missing\n",
            "  - M5: 59.35% missing\n",
            "  - M7: 58.64% missing\n",
            "  - M8: 58.63% missing\n",
            "  - M9: 58.63% missing\n",
            "  - V138: 86.12% missing\n",
            "  - V139: 86.12% missing\n",
            "  - V140: 86.12% missing\n",
            "  - V141: 86.12% missing\n",
            "  - V142: 86.12% missing\n",
            "  ... and 154 more\n",
            "\n",
            "âœ“ Dropped 174 columns\n",
            "Current shape: (590540, 220)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPUTE MISSING VALUES**"
      ],
      "metadata": {
        "id": "fSlB5RcsXtJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 3: IMPUTING MISSING VALUES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Numeric columns: Impute with median\n",
        "imputation_stats = {}\n",
        "\n",
        "print(\"\\nImputing Numeric Columns with Median...\")\n",
        "for col in numeric_cols:\n",
        "    if train_df[col].isnull().sum() > 0:\n",
        "        median_value = train_df[col].median()\n",
        "        missing_count = train_df[col].isnull().sum()\n",
        "        train_df[col].fillna(median_value, inplace=True)\n",
        "        imputation_stats[col] = {'type': 'numeric', 'method': 'median',\n",
        "                                  'value': median_value, 'filled': missing_count}\n",
        "        print(f\"  âœ“ {col}: filled {missing_count} values with median {median_value:.2f}\")\n",
        "\n",
        "print(f\"\\nImputed {len([k for k, v in imputation_stats.items() if v['type'] == 'numeric'])} numeric columns\")\n",
        "\n",
        "# Categorical columns: Impute with mode or 'Unknown'\n",
        "print(\"\\nImputing Categorical Columns with Mode...\")\n",
        "for col in categorical_cols:\n",
        "    if train_df[col].isnull().sum() > 0:\n",
        "        missing_count = train_df[col].isnull().sum()\n",
        "        if train_df[col].mode().shape[0] > 0:\n",
        "            mode_value = train_df[col].mode()[0]\n",
        "        else:\n",
        "            mode_value = 'Unknown'\n",
        "        train_df[col].fillna(mode_value, inplace=True)\n",
        "        imputation_stats[col] = {'type': 'categorical', 'method': 'mode',\n",
        "                                  'value': mode_value, 'filled': missing_count}\n",
        "        print(f\"  âœ“ {col}: filled {missing_count} values with '{mode_value}'\")\n",
        "\n",
        "print(f\"\\nImputed {len([k for k, v in imputation_stats.items() if v['type'] == 'categorical'])} categorical columns\")\n",
        "\n",
        "# Save imputation statistics\n",
        "with open('results/02_imputation_stats.pkl', 'wb') as f:\n",
        "    pickle.dump(imputation_stats, f)\n",
        "\n",
        "# Verify no missing values remain\n",
        "remaining_missing = train_df.isnull().sum().sum()\n",
        "print(f\"\\nâœ“ Total remaining missing values: {remaining_missing}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL-BNpTfXuO9",
        "outputId": "c0528b1f-6239-456c-809e-46552a9c497c"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 3: IMPUTING MISSING VALUES\n",
            "============================================================\n",
            "\n",
            "Imputing Numeric Columns with Median...\n",
            "  âœ“ card2: filled 8933 values with median 361.00\n",
            "  âœ“ card3: filled 1565 values with median 150.00\n",
            "  âœ“ card5: filled 4259 values with median 226.00\n",
            "  âœ“ addr1: filled 65706 values with median 299.00\n",
            "  âœ“ addr2: filled 65706 values with median 87.00\n",
            "  âœ“ D1: filled 1269 values with median 3.00\n",
            "  âœ“ D2: filled 280797 values with median 97.00\n",
            "  âœ“ D3: filled 262878 values with median 8.00\n",
            "  âœ“ D4: filled 168922 values with median 26.00\n",
            "  âœ“ D10: filled 76022 values with median 15.00\n",
            "  âœ“ D11: filled 279287 values with median 43.00\n",
            "  âœ“ D15: filled 89113 values with median 52.00\n",
            "  âœ“ V1: filled 279287 values with median 1.00\n",
            "  âœ“ V2: filled 279287 values with median 1.00\n",
            "  âœ“ V3: filled 279287 values with median 1.00\n",
            "  âœ“ V4: filled 279287 values with median 1.00\n",
            "  âœ“ V5: filled 279287 values with median 1.00\n",
            "  âœ“ V6: filled 279287 values with median 1.00\n",
            "  âœ“ V7: filled 279287 values with median 1.00\n",
            "  âœ“ V8: filled 279287 values with median 1.00\n",
            "  âœ“ V9: filled 279287 values with median 1.00\n",
            "  âœ“ V10: filled 279287 values with median 0.00\n",
            "  âœ“ V11: filled 279287 values with median 0.00\n",
            "  âœ“ V12: filled 76073 values with median 1.00\n",
            "  âœ“ V13: filled 76073 values with median 1.00\n",
            "  âœ“ V14: filled 76073 values with median 1.00\n",
            "  âœ“ V15: filled 76073 values with median 0.00\n",
            "  âœ“ V16: filled 76073 values with median 0.00\n",
            "  âœ“ V17: filled 76073 values with median 0.00\n",
            "  âœ“ V18: filled 76073 values with median 0.00\n",
            "  âœ“ V19: filled 76073 values with median 1.00\n",
            "  âœ“ V20: filled 76073 values with median 1.00\n",
            "  âœ“ V21: filled 76073 values with median 0.00\n",
            "  âœ“ V22: filled 76073 values with median 0.00\n",
            "  âœ“ V23: filled 76073 values with median 1.00\n",
            "  âœ“ V24: filled 76073 values with median 1.00\n",
            "  âœ“ V25: filled 76073 values with median 1.00\n",
            "  âœ“ V26: filled 76073 values with median 1.00\n",
            "  âœ“ V27: filled 76073 values with median 0.00\n",
            "  âœ“ V28: filled 76073 values with median 0.00\n",
            "  âœ“ V29: filled 76073 values with median 0.00\n",
            "  âœ“ V30: filled 76073 values with median 0.00\n",
            "  âœ“ V31: filled 76073 values with median 0.00\n",
            "  âœ“ V32: filled 76073 values with median 0.00\n",
            "  âœ“ V33: filled 76073 values with median 0.00\n",
            "  âœ“ V34: filled 76073 values with median 0.00\n",
            "  âœ“ V35: filled 168969 values with median 1.00\n",
            "  âœ“ V36: filled 168969 values with median 1.00\n",
            "  âœ“ V37: filled 168969 values with median 1.00\n",
            "  âœ“ V38: filled 168969 values with median 1.00\n",
            "  âœ“ V39: filled 168969 values with median 0.00\n",
            "  âœ“ V40: filled 168969 values with median 0.00\n",
            "  âœ“ V41: filled 168969 values with median 1.00\n",
            "  âœ“ V42: filled 168969 values with median 0.00\n",
            "  âœ“ V43: filled 168969 values with median 0.00\n",
            "  âœ“ V44: filled 168969 values with median 1.00\n",
            "  âœ“ V45: filled 168969 values with median 1.00\n",
            "  âœ“ V46: filled 168969 values with median 1.00\n",
            "  âœ“ V47: filled 168969 values with median 1.00\n",
            "  âœ“ V48: filled 168969 values with median 0.00\n",
            "  âœ“ V49: filled 168969 values with median 0.00\n",
            "  âœ“ V50: filled 168969 values with median 0.00\n",
            "  âœ“ V51: filled 168969 values with median 0.00\n",
            "  âœ“ V52: filled 168969 values with median 0.00\n",
            "  âœ“ V53: filled 77096 values with median 1.00\n",
            "  âœ“ V54: filled 77096 values with median 1.00\n",
            "  âœ“ V55: filled 77096 values with median 1.00\n",
            "  âœ“ V56: filled 77096 values with median 1.00\n",
            "  âœ“ V57: filled 77096 values with median 0.00\n",
            "  âœ“ V58: filled 77096 values with median 0.00\n",
            "  âœ“ V59: filled 77096 values with median 0.00\n",
            "  âœ“ V60: filled 77096 values with median 0.00\n",
            "  âœ“ V61: filled 77096 values with median 1.00\n",
            "  âœ“ V62: filled 77096 values with median 1.00\n",
            "  âœ“ V63: filled 77096 values with median 0.00\n",
            "  âœ“ V64: filled 77096 values with median 0.00\n",
            "  âœ“ V65: filled 77096 values with median 1.00\n",
            "  âœ“ V66: filled 77096 values with median 1.00\n",
            "  âœ“ V67: filled 77096 values with median 1.00\n",
            "  âœ“ V68: filled 77096 values with median 0.00\n",
            "  âœ“ V69: filled 77096 values with median 0.00\n",
            "  âœ“ V70: filled 77096 values with median 0.00\n",
            "  âœ“ V71: filled 77096 values with median 0.00\n",
            "  âœ“ V72: filled 77096 values with median 0.00\n",
            "  âœ“ V73: filled 77096 values with median 0.00\n",
            "  âœ“ V74: filled 77096 values with median 0.00\n",
            "  âœ“ V75: filled 89164 values with median 1.00\n",
            "  âœ“ V76: filled 89164 values with median 1.00\n",
            "  âœ“ V77: filled 89164 values with median 1.00\n",
            "  âœ“ V78: filled 89164 values with median 1.00\n",
            "  âœ“ V79: filled 89164 values with median 0.00\n",
            "  âœ“ V80: filled 89164 values with median 0.00\n",
            "  âœ“ V81: filled 89164 values with median 0.00\n",
            "  âœ“ V82: filled 89164 values with median 1.00\n",
            "  âœ“ V83: filled 89164 values with median 1.00\n",
            "  âœ“ V84: filled 89164 values with median 0.00\n",
            "  âœ“ V85: filled 89164 values with median 0.00\n",
            "  âœ“ V86: filled 89164 values with median 1.00\n",
            "  âœ“ V87: filled 89164 values with median 1.00\n",
            "  âœ“ V88: filled 89164 values with median 1.00\n",
            "  âœ“ V89: filled 89164 values with median 0.00\n",
            "  âœ“ V90: filled 89164 values with median 0.00\n",
            "  âœ“ V91: filled 89164 values with median 0.00\n",
            "  âœ“ V92: filled 89164 values with median 0.00\n",
            "  âœ“ V93: filled 89164 values with median 0.00\n",
            "  âœ“ V94: filled 89164 values with median 0.00\n",
            "  âœ“ V95: filled 314 values with median 0.00\n",
            "  âœ“ V96: filled 314 values with median 0.00\n",
            "  âœ“ V97: filled 314 values with median 0.00\n",
            "  âœ“ V98: filled 314 values with median 0.00\n",
            "  âœ“ V99: filled 314 values with median 0.00\n",
            "  âœ“ V100: filled 314 values with median 0.00\n",
            "  âœ“ V101: filled 314 values with median 0.00\n",
            "  âœ“ V102: filled 314 values with median 0.00\n",
            "  âœ“ V103: filled 314 values with median 0.00\n",
            "  âœ“ V104: filled 314 values with median 0.00\n",
            "  âœ“ V105: filled 314 values with median 0.00\n",
            "  âœ“ V106: filled 314 values with median 0.00\n",
            "  âœ“ V107: filled 314 values with median 1.00\n",
            "  âœ“ V108: filled 314 values with median 1.00\n",
            "  âœ“ V109: filled 314 values with median 1.00\n",
            "  âœ“ V110: filled 314 values with median 1.00\n",
            "  âœ“ V111: filled 314 values with median 1.00\n",
            "  âœ“ V112: filled 314 values with median 1.00\n",
            "  âœ“ V113: filled 314 values with median 1.00\n",
            "  âœ“ V114: filled 314 values with median 1.00\n",
            "  âœ“ V115: filled 314 values with median 1.00\n",
            "  âœ“ V116: filled 314 values with median 1.00\n",
            "  âœ“ V117: filled 314 values with median 1.00\n",
            "  âœ“ V118: filled 314 values with median 1.00\n",
            "  âœ“ V119: filled 314 values with median 1.00\n",
            "  âœ“ V120: filled 314 values with median 1.00\n",
            "  âœ“ V121: filled 314 values with median 1.00\n",
            "  âœ“ V122: filled 314 values with median 1.00\n",
            "  âœ“ V123: filled 314 values with median 1.00\n",
            "  âœ“ V124: filled 314 values with median 1.00\n",
            "  âœ“ V125: filled 314 values with median 1.00\n",
            "  âœ“ V126: filled 314 values with median 0.00\n",
            "  âœ“ V127: filled 314 values with median 0.00\n",
            "  âœ“ V128: filled 314 values with median 0.00\n",
            "  âœ“ V129: filled 314 values with median 0.00\n",
            "  âœ“ V130: filled 314 values with median 0.00\n",
            "  âœ“ V131: filled 314 values with median 0.00\n",
            "  âœ“ V132: filled 314 values with median 0.00\n",
            "  âœ“ V133: filled 314 values with median 0.00\n",
            "  âœ“ V134: filled 314 values with median 0.00\n",
            "  âœ“ V135: filled 314 values with median 0.00\n",
            "  âœ“ V136: filled 314 values with median 0.00\n",
            "  âœ“ V137: filled 314 values with median 0.00\n",
            "  âœ“ V279: filled 12 values with median 0.00\n",
            "  âœ“ V280: filled 12 values with median 0.00\n",
            "  âœ“ V281: filled 1269 values with median 0.00\n",
            "  âœ“ V282: filled 1269 values with median 1.00\n",
            "  âœ“ V283: filled 1269 values with median 1.00\n",
            "  âœ“ V284: filled 12 values with median 0.00\n",
            "  âœ“ V285: filled 12 values with median 0.00\n",
            "  âœ“ V286: filled 12 values with median 0.00\n",
            "  âœ“ V287: filled 12 values with median 0.00\n",
            "  âœ“ V288: filled 1269 values with median 0.00\n",
            "  âœ“ V289: filled 1269 values with median 0.00\n",
            "  âœ“ V290: filled 12 values with median 1.00\n",
            "  âœ“ V291: filled 12 values with median 1.00\n",
            "  âœ“ V292: filled 12 values with median 1.00\n",
            "  âœ“ V293: filled 12 values with median 0.00\n",
            "  âœ“ V294: filled 12 values with median 0.00\n",
            "  âœ“ V295: filled 12 values with median 0.00\n",
            "  âœ“ V296: filled 1269 values with median 0.00\n",
            "  âœ“ V297: filled 12 values with median 0.00\n",
            "  âœ“ V298: filled 12 values with median 0.00\n",
            "  âœ“ V299: filled 12 values with median 0.00\n",
            "  âœ“ V300: filled 1269 values with median 0.00\n",
            "  âœ“ V301: filled 1269 values with median 0.00\n",
            "  âœ“ V302: filled 12 values with median 0.00\n",
            "  âœ“ V303: filled 12 values with median 0.00\n",
            "  âœ“ V304: filled 12 values with median 0.00\n",
            "  âœ“ V305: filled 12 values with median 1.00\n",
            "  âœ“ V306: filled 12 values with median 0.00\n",
            "  âœ“ V307: filled 12 values with median 0.00\n",
            "  âœ“ V308: filled 12 values with median 0.00\n",
            "  âœ“ V309: filled 12 values with median 0.00\n",
            "  âœ“ V310: filled 12 values with median 0.00\n",
            "  âœ“ V311: filled 12 values with median 0.00\n",
            "  âœ“ V312: filled 12 values with median 0.00\n",
            "  âœ“ V313: filled 1269 values with median 0.00\n",
            "  âœ“ V314: filled 1269 values with median 0.00\n",
            "  âœ“ V315: filled 1269 values with median 0.00\n",
            "  âœ“ V316: filled 12 values with median 0.00\n",
            "  âœ“ V317: filled 12 values with median 0.00\n",
            "  âœ“ V318: filled 12 values with median 0.00\n",
            "  âœ“ V319: filled 12 values with median 0.00\n",
            "  âœ“ V320: filled 12 values with median 0.00\n",
            "  âœ“ V321: filled 12 values with median 0.00\n",
            "\n",
            "Imputed 192 numeric columns\n",
            "\n",
            "Imputing Categorical Columns with Mode...\n",
            "  âœ“ card4: filled 1577 values with 'visa'\n",
            "  âœ“ card6: filled 1571 values with 'debit'\n",
            "  âœ“ P_emaildomain: filled 94456 values with 'gmail.com'\n",
            "  âœ“ M1: filled 271100 values with 'T'\n",
            "  âœ“ M2: filled 271100 values with 'T'\n",
            "  âœ“ M3: filled 271100 values with 'T'\n",
            "  âœ“ M4: filled 281444 values with 'M0'\n",
            "  âœ“ M6: filled 169360 values with 'F'\n",
            "\n",
            "Imputed 8 categorical columns\n",
            "\n",
            "âœ“ Total remaining missing values: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**HANDLE OUTLIERS (CAPPING METHOD)**"
      ],
      "metadata": {
        "id": "07rTaQKbXz-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 4: HANDLING OUTLIERS (IQR CAPPING)\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "outlier_caps = {}\n",
        "\n",
        "print(\"\\nCapping outliers for numeric columns...\")\n",
        "for col in numeric_cols[:10]:  # Process first 10 to save time, adjust as needed\n",
        "    Q1 = train_df[col].quantile(0.25)\n",
        "    Q3 = train_df[col].quantile(0.75)\n",
        "    IQR = Q3 - Q1\n",
        "    lower_bound = Q1 - 1.5 * IQR\n",
        "    upper_bound = Q3 + 1.5 * IQR\n",
        "\n",
        "    # Count outliers before capping\n",
        "    outliers_count = ((train_df[col] < lower_bound) | (train_df[col] > upper_bound)).sum()\n",
        "\n",
        "    if outliers_count > 0:\n",
        "        # Cap outliers\n",
        "        train_df[col] = train_df[col].clip(lower=lower_bound, upper=upper_bound)\n",
        "        outlier_caps[col] = {'lower': lower_bound, 'upper': upper_bound, 'capped': outliers_count}\n",
        "        print(f\"  âœ“ {col}: capped {outliers_count} outliers\")\n",
        "\n",
        "print(f\"\\nâœ“ Processed outliers for {len(outlier_caps)} columns\")\n",
        "\n",
        "# Save outlier caps for test data\n",
        "with open('results/02_outlier_caps.pkl', 'wb') as f:\n",
        "    pickle.dump(outlier_caps, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya3Ci8_uX03W",
        "outputId": "7ece72fb-cb75-4f9a-96fc-168471aed743"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 4: HANDLING OUTLIERS (IQR CAPPING)\n",
            "============================================================\n",
            "\n",
            "Capping outliers for numeric columns...\n",
            "  âœ“ TransactionAmt: capped 66482 outliers\n",
            "  âœ“ card3: capped 67688 outliers\n",
            "  âœ“ addr1: capped 8807 outliers\n",
            "  âœ“ addr2: capped 4353 outliers\n",
            "  âœ“ C1: capped 59535 outliers\n",
            "\n",
            "âœ“ Processed outliers for 5 columns\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**OPTIMIZE DATA TYPES (MEMORY REDUCTION)**"
      ],
      "metadata": {
        "id": "tgBJMH2qX6zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"STEP 5: OPTIMIZING DATA TYPES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "memory_before = train_df.memory_usage(deep=True).sum() / 1024**2\n",
        "\n",
        "# Optimize numeric columns\n",
        "for col in numeric_cols:\n",
        "    col_type = train_df[col].dtype\n",
        "\n",
        "    if col_type == 'int64':\n",
        "        c_min = train_df[col].min()\n",
        "        c_max = train_df[col].max()\n",
        "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
        "            train_df[col] = train_df[col].astype(np.int8)\n",
        "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
        "            train_df[col] = train_df[col].astype(np.int16)\n",
        "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
        "            train_df[col] = train_df[col].astype(np.int32)\n",
        "\n",
        "    elif col_type == 'float64':\n",
        "        train_df[col] = train_df[col].astype(np.float32)\n",
        "\n",
        "# Optimize categorical columns with limited unique values\n",
        "for col in categorical_cols:\n",
        "    num_unique_values = train_df[col].nunique()\n",
        "    num_total_values = len(train_df[col])\n",
        "    if num_unique_values / num_total_values < 0.5:  # If less than 50% unique\n",
        "        train_df[col] = train_df[col].astype('category')\n",
        "\n",
        "memory_after = train_df.memory_usage(deep=True).sum() / 1024**2\n",
        "memory_saved = memory_before - memory_after\n",
        "memory_reduction_pct = (memory_saved / memory_before) * 100\n",
        "\n",
        "print(f\"Memory before: {memory_before:.2f} MB\")\n",
        "print(f\"Memory after: {memory_after:.2f} MB\")\n",
        "print(f\"Memory saved: {memory_saved:.2f} MB ({memory_reduction_pct:.2f}%)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnHYZ4DjX7x0",
        "outputId": "5be3b2fd-96b8-446e-ad12-668633d52dc4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "STEP 5: OPTIMIZING DATA TYPES\n",
            "============================================================\n",
            "Memory before: 1214.67 MB\n",
            "Memory after: 481.53 MB\n",
            "Memory saved: 733.14 MB (60.36%)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE CLEANED DATA**"
      ],
      "metadata": {
        "id": "tbFx_MZQYBnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING CLEANED DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save cleaned data\n",
        "train_df.to_csv('data/cleaned/train_cleaned.csv', index=False)\n",
        "print(f\"ðŸ’¾ Saved: data/cleaned/train_cleaned.csv\")\n",
        "print(f\"   Shape: {train_df.shape}\")\n",
        "print(f\"   Size: {os.path.getsize('data/cleaned/train_cleaned.csv') / 1024**2:.2f} MB\")\n",
        "\n",
        "# Update and save column info\n",
        "updated_column_info = {\n",
        "    'numeric_columns': [col for col in numeric_cols if col in train_df.columns],\n",
        "    'categorical_columns': [col for col in categorical_cols if col in train_df.columns],\n",
        "    'all_columns': train_df.columns.tolist(),\n",
        "    'target_column': 'isFraud'\n",
        "}\n",
        "\n",
        "with open('results/02_updated_column_info.pkl', 'wb') as f:\n",
        "    pickle.dump(updated_column_info, f)\n",
        "\n",
        "print(\"ðŸ’¾ Saved: results/02_updated_column_info.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uciAQwwzYCgS",
        "outputId": "0901fa39-342a-4f2c-de7c-08a1417b8822"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING CLEANED DATA\n",
            "============================================================\n",
            "ðŸ’¾ Saved: data/cleaned/train_cleaned.csv\n",
            "   Shape: (590540, 220)\n",
            "   Size: 515.95 MB\n",
            "ðŸ’¾ Saved: results/02_updated_column_info.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**PREPROCESSING SUMMARY**"
      ],
      "metadata": {
        "id": "3Iz3YepvYFfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"PREPROCESSING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = f\"\"\"\n",
        "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
        "\n",
        "Original Shape: {initial_rows} rows Ã— {len(column_info['all_columns'])} columns\n",
        "Final Shape: {train_df.shape[0]} rows Ã— {train_df.shape[1]} columns\n",
        "\n",
        "Steps Completed:\n",
        "1. âœ“ Removed {removed_duplicates} duplicate rows\n",
        "2. âœ“ Dropped {len(cols_to_drop)} columns with >50% missing values\n",
        "3. âœ“ Imputed {len(imputation_stats)} columns with missing values\n",
        "4. âœ“ Capped outliers in {len(outlier_caps)} numeric columns\n",
        "5. âœ“ Optimized data types (saved {memory_saved:.2f} MB)\n",
        "\n",
        "Final Dataset:\n",
        "- Numeric columns: {len(updated_column_info['numeric_columns'])}\n",
        "- Categorical columns: {len(updated_column_info['categorical_columns'])}\n",
        "- Total columns: {len(updated_column_info['all_columns'])}\n",
        "- Memory usage: {memory_after:.2f} MB\n",
        "- No missing values: {train_df.isnull().sum().sum() == 0}\n",
        "\n",
        "Next Step: Proceed to Notebook 3 for Feature Engineering\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary\n",
        "with open('results/02_preprocessing_summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"ðŸ’¾ Saved: results/02_preprocessing_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNue11w1YHk4",
        "outputId": "5680f0aa-64be-455a-f351-258a4f9bfb5f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "PREPROCESSING SUMMARY\n",
            "============================================================\n",
            "\n",
            "PREPROCESSING COMPLETED SUCCESSFULLY!\n",
            "\n",
            "Original Shape: 590540 rows Ã— 394 columns\n",
            "Final Shape: 590540 rows Ã— 220 columns\n",
            "\n",
            "Steps Completed:\n",
            "1. âœ“ Removed 0 duplicate rows\n",
            "2. âœ“ Dropped 174 columns with >50% missing values\n",
            "3. âœ“ Imputed 200 columns with missing values\n",
            "4. âœ“ Capped outliers in 5 numeric columns\n",
            "5. âœ“ Optimized data types (saved 733.14 MB)\n",
            "\n",
            "Final Dataset:\n",
            "- Numeric columns: 210\n",
            "- Categorical columns: 9\n",
            "- Total columns: 220\n",
            "- Memory usage: 481.53 MB\n",
            "- No missing values: True\n",
            "\n",
            "Next Step: Proceed to Notebook 3 for Feature Engineering\n",
            "\n",
            "ðŸ’¾ Saved: results/02_preprocessing_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MEMORY CLEANUP**"
      ],
      "metadata": {
        "id": "AmMMxxrhYKpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MEMORY CLEANUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "del column_info, imputation_stats, outlier_caps\n",
        "gc.collect()\n",
        "\n",
        "print(\"âœ“ Memory cleaned\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK 2 COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiWJ95QdYMhj",
        "outputId": "c6fd421d-1eb4-40c8-a6f2-70e17d13bd83"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MEMORY CLEANUP\n",
            "============================================================\n",
            "âœ“ Memory cleaned\n",
            "\n",
            "============================================================\n",
            "NOTEBOOK 2 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}