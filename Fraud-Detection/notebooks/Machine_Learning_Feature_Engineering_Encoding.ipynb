{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import Libraries**"
      ],
      "metadata": {
        "id": "iJhkfjDjWWbX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "9RhL5Cb5WMUZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pickle\n",
        "import gc\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MOUNT GOOGLE DRIVE**"
      ],
      "metadata": {
        "id": "jRTi0-wxW209"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "project_folder = '/content/drive/MyDrive/FraudDetection'  # Sesuaikan dengan folder kamu\n",
        "os.chdir(project_folder)\n",
        "\n",
        "print(f\"Working Directory: {os.getcwd()}\")\n",
        "print(\"\\nâœ… Google Drive mounted!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tv3hTXDBW5DU",
        "outputId": "48fe64d4-c59f-4c43-8296-c26eeda49492"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Working Directory: /content/drive/MyDrive/FraudDetection\n",
            "\n",
            "âœ… Google Drive mounted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LOAD DATA & COLUMN INFO**"
      ],
      "metadata": {
        "id": "Cv2NnEavXIBH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"LOADING CLEANED DATA\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Load cleaned data from Notebook 2\n",
        "train_df = pd.read_csv('data/cleaned/train_cleaned.csv')\n",
        "print(f\"Dataset Shape: {train_df.shape}\")\n",
        "print(f\"Memory Usage: {train_df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
        "\n",
        "# Load column info\n",
        "with open('results/02_updated_column_info.pkl', 'rb') as f:\n",
        "    column_info = pickle.load(f)\n",
        "\n",
        "numeric_cols = column_info['numeric_columns']\n",
        "categorical_cols = column_info['categorical_columns']\n",
        "\n",
        "print(f\"\\nNumeric Columns: {len(numeric_cols)}\")\n",
        "print(f\"Categorical Columns: {len(categorical_cols)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bxzLcEq6XJgM",
        "outputId": "5fd393ad-a1ec-467d-cd36-8bd784ff7976"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "LOADING CLEANED DATA\n",
            "============================================================\n",
            "Dataset Shape: (590540, 220)\n",
            "Memory Usage: 1214.67 MB\n",
            "\n",
            "Numeric Columns: 210\n",
            "Categorical Columns: 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SEPARATE TARGET VARIABLE**"
      ],
      "metadata": {
        "id": "CTfNbOc0Xcae"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SEPARATING FEATURES AND TARGET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Check if TransactionID exists and store it\n",
        "if 'TransactionID' in train_df.columns:\n",
        "    transaction_ids = train_df['TransactionID'].copy()\n",
        "    train_df = train_df.drop('TransactionID', axis=1)\n",
        "    print(\"âœ“ Stored TransactionID for later use\")\n",
        "\n",
        "    # Remove from column lists\n",
        "    if 'TransactionID' in numeric_cols:\n",
        "        numeric_cols.remove('TransactionID')\n",
        "    if 'TransactionID' in categorical_cols:\n",
        "        categorical_cols.remove('TransactionID')\n",
        "\n",
        "# Separate target\n",
        "y = train_df['isFraud'].copy()\n",
        "X = train_df.drop('isFraud', axis=1)\n",
        "\n",
        "print(f\"\\nFeatures (X): {X.shape}\")\n",
        "print(f\"Target (y): {y.shape}\")\n",
        "print(f\"\\nTarget Distribution:\")\n",
        "print(y.value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1CPCcR6XddN",
        "outputId": "22558327-877a-48a5-b6fd-98da49ec16bf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SEPARATING FEATURES AND TARGET\n",
            "============================================================\n",
            "âœ“ Stored TransactionID for later use\n",
            "\n",
            "Features (X): (590540, 218)\n",
            "Target (y): (590540,)\n",
            "\n",
            "Target Distribution:\n",
            "isFraud\n",
            "0    569877\n",
            "1     20663\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**CREATE NEW FEATURES (IF APPLICABLE)**"
      ],
      "metadata": {
        "id": "6jKXVtd-XkAT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING - CREATING NEW FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "new_features_created = []\n",
        "\n",
        "# Example: If there's a timestamp column, extract time-based features\n",
        "# Adjust based on your actual column names\n",
        "time_cols = [col for col in X.columns if 'time' in col.lower() or 'date' in col.lower()]\n",
        "\n",
        "if len(time_cols) > 0:\n",
        "    print(f\"\\nFound {len(time_cols)} time-related columns: {time_cols[:5]}\")\n",
        "\n",
        "    # Example feature engineering (uncomment and adjust as needed)\n",
        "    # for col in time_cols[:1]:  # Process first time column as example\n",
        "    #     if X[col].dtype in ['int64', 'float64']:\n",
        "    #         # Create hour, day features if it's a timestamp\n",
        "    #         X[f'{col}_hour'] = (X[col] // 3600) % 24\n",
        "    #         X[f'{col}_day'] = (X[col] // 86400) % 7\n",
        "    #         new_features_created.extend([f'{col}_hour', f'{col}_day'])\n",
        "    #         numeric_cols.extend([f'{col}_hour', f'{col}_day'])\n",
        "\n",
        "    print(f\"âœ“ Time-based features can be engineered here if needed\")\n",
        "else:\n",
        "    print(\"âœ“ No obvious time columns found\")\n",
        "\n",
        "# Example: Amount-based features (if applicable)\n",
        "amount_cols = [col for col in numeric_cols if 'amount' in col.lower() or 'transactionamt' in col.lower()]\n",
        "\n",
        "if len(amount_cols) > 0:\n",
        "    print(f\"\\nFound {len(amount_cols)} amount-related columns\")\n",
        "    # You can create log transforms, bins, etc.\n",
        "    # for col in amount_cols[:1]:\n",
        "    #     X[f'{col}_log'] = np.log1p(X[col])\n",
        "    #     new_features_created.append(f'{col}_log')\n",
        "    #     numeric_cols.append(f'{col}_log')\n",
        "    print(f\"âœ“ Amount-based features can be engineered here if needed\")\n",
        "\n",
        "print(f\"\\nNew features created: {len(new_features_created)}\")\n",
        "if len(new_features_created) > 0:\n",
        "    print(f\"Feature names: {new_features_created}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PiukuKyGXlkE",
        "outputId": "c8325ed5-7b19-4c35-ab67-e282515a07c3"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE ENGINEERING - CREATING NEW FEATURES\n",
            "============================================================\n",
            "âœ“ No obvious time columns found\n",
            "\n",
            "Found 1 amount-related columns\n",
            "âœ“ Amount-based features can be engineered here if needed\n",
            "\n",
            "New features created: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ENCODE CATEGORICAL VARIABLES**"
      ],
      "metadata": {
        "id": "fSlB5RcsXtJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"ENCODING CATEGORICAL VARIABLES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Filter categorical columns that still exist in X\n",
        "categorical_cols = [col for col in categorical_cols if col in X.columns]\n",
        "\n",
        "print(f\"\\nCategorical columns to encode: {len(categorical_cols)}\")\n",
        "\n",
        "label_encoders = {}\n",
        "encoding_mappings = {}\n",
        "\n",
        "if len(categorical_cols) > 0:\n",
        "    print(\"\\nEncoding categorical columns with Label Encoding...\")\n",
        "\n",
        "    for i, col in enumerate(categorical_cols, 1):\n",
        "        if i % 10 == 0:\n",
        "            print(f\"  Processed {i}/{len(categorical_cols)} columns...\")\n",
        "\n",
        "        # Convert to string type first\n",
        "        X[col] = X[col].astype(str)\n",
        "\n",
        "        # Initialize and fit label encoder\n",
        "        le = LabelEncoder()\n",
        "        X[col] = le.fit_transform(X[col])\n",
        "\n",
        "        # Store encoder and mapping\n",
        "        label_encoders[col] = le\n",
        "        encoding_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))\n",
        "\n",
        "    print(f\"\\nâœ“ Encoded {len(categorical_cols)} categorical columns\")\n",
        "\n",
        "    # Show sample encoding\n",
        "    sample_col = categorical_cols[0]\n",
        "    print(f\"\\nSample encoding for '{sample_col}':\")\n",
        "    sample_mapping = list(encoding_mappings[sample_col].items())[:5]\n",
        "    for original, encoded in sample_mapping:\n",
        "        print(f\"  '{original}' â†’ {encoded}\")\n",
        "    if len(encoding_mappings[sample_col]) > 5:\n",
        "        print(f\"  ... and {len(encoding_mappings[sample_col]) - 5} more mappings\")\n",
        "else:\n",
        "    print(\"âœ“ No categorical columns to encode\")\n",
        "\n",
        "# Save encoders for test data\n",
        "with open('results/03_label_encoders.pkl', 'wb') as f:\n",
        "    pickle.dump(label_encoders, f)\n",
        "\n",
        "print(\"\\nðŸ’¾ Saved: results/03_label_encoders.pkl\")\n",
        "\n",
        "# Save encoding mappings for reference\n",
        "with open('results/03_encoding_mappings.pkl', 'wb') as f:\n",
        "    pickle.dump(encoding_mappings, f)\n",
        "\n",
        "print(\"ðŸ’¾ Saved: results/03_encoding_mappings.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BL-BNpTfXuO9",
        "outputId": "78c33a49-455e-4d7f-ccf3-a17cbb7521b6"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "ENCODING CATEGORICAL VARIABLES\n",
            "============================================================\n",
            "\n",
            "Categorical columns to encode: 9\n",
            "\n",
            "Encoding categorical columns with Label Encoding...\n",
            "\n",
            "âœ“ Encoded 9 categorical columns\n",
            "\n",
            "Sample encoding for 'ProductCD':\n",
            "  'C' â†’ 0\n",
            "  'H' â†’ 1\n",
            "  'R' â†’ 2\n",
            "  'S' â†’ 3\n",
            "  'W' â†’ 4\n",
            "\n",
            "ðŸ’¾ Saved: results/03_label_encoders.pkl\n",
            "ðŸ’¾ Saved: results/03_encoding_mappings.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE SELECTION (CORRELATION-BASED)**"
      ],
      "metadata": {
        "id": "07rTaQKbXz-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE SELECTION - REMOVING HIGH CORRELATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Calculate correlation matrix (for numeric features only)\n",
        "print(\"\\nCalculating correlation matrix...\")\n",
        "correlation_matrix = X.corr().abs()\n",
        "\n",
        "# Find features with correlation > 0.95\n",
        "upper_triangle = correlation_matrix.where(\n",
        "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
        ")\n",
        "\n",
        "high_corr_features = [column for column in upper_triangle.columns\n",
        "                      if any(upper_triangle[column] > 0.95)]\n",
        "\n",
        "print(f\"\\nFeatures with correlation > 0.95: {len(high_corr_features)}\")\n",
        "\n",
        "if len(high_corr_features) > 0:\n",
        "    print(f\"\\nRemoving {len(high_corr_features)} highly correlated features...\")\n",
        "    for feat in high_corr_features[:10]:  # Show first 10\n",
        "        print(f\"  - {feat}\")\n",
        "    if len(high_corr_features) > 10:\n",
        "        print(f\"  ... and {len(high_corr_features) - 10} more\")\n",
        "\n",
        "    X = X.drop(columns=high_corr_features)\n",
        "\n",
        "    # Update column lists\n",
        "    numeric_cols = [col for col in numeric_cols if col not in high_corr_features]\n",
        "\n",
        "    print(f\"\\nâœ“ Removed highly correlated features\")\n",
        "else:\n",
        "    print(\"âœ“ No highly correlated features found (threshold: 0.95)\")\n",
        "\n",
        "print(f\"\\nFinal feature count: {X.shape[1]}\")\n",
        "\n",
        "# Save list of removed features\n",
        "with open('results/03_removed_corr_features.pkl', 'wb') as f:\n",
        "    pickle.dump(high_corr_features, f)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ya3Ci8_uX03W",
        "outputId": "ae338c69-2619-4b62-a51c-afad0094b3ed"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE SELECTION - REMOVING HIGH CORRELATION\n",
            "============================================================\n",
            "\n",
            "Calculating correlation matrix...\n",
            "\n",
            "Features with correlation > 0.95: 55\n",
            "\n",
            "Removing 55 highly correlated features...\n",
            "  - C4\n",
            "  - C6\n",
            "  - C8\n",
            "  - C10\n",
            "  - C11\n",
            "  - C12\n",
            "  - C14\n",
            "  - V11\n",
            "  - V16\n",
            "  - V18\n",
            "  ... and 45 more\n",
            "\n",
            "âœ“ Removed highly correlated features\n",
            "\n",
            "Final feature count: 163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**VERIFY DATA QUALITY**"
      ],
      "metadata": {
        "id": "tgBJMH2qX6zk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"DATA QUALITY VERIFICATION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nChecking for issues...\")\n",
        "\n",
        "# Check for missing values\n",
        "missing_count = X.isnull().sum().sum()\n",
        "print(f\"âœ“ Missing values: {missing_count}\")\n",
        "\n",
        "# Check for infinite values\n",
        "inf_count = np.isinf(X.select_dtypes(include=[np.number])).sum().sum()\n",
        "print(f\"âœ“ Infinite values: {inf_count}\")\n",
        "\n",
        "# Check data types\n",
        "print(f\"âœ“ All numeric: {X.select_dtypes(include=[np.number]).shape[1] == X.shape[1]}\")\n",
        "\n",
        "# Check shape consistency\n",
        "print(f\"âœ“ X shape: {X.shape}\")\n",
        "print(f\"âœ“ y shape: {y.shape}\")\n",
        "print(f\"âœ“ Shapes match: {X.shape[0] == y.shape[0]}\")\n",
        "\n",
        "if missing_count > 0 or inf_count > 0:\n",
        "    print(\"\\nâš ï¸ Warning: Data quality issues detected!\")\n",
        "else:\n",
        "    print(\"\\nâœ… Data quality verified - Ready for model training!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vnHYZ4DjX7x0",
        "outputId": "bf69ef89-e615-4f6f-edda-b0d5d9f52f4a"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "DATA QUALITY VERIFICATION\n",
            "============================================================\n",
            "\n",
            "Checking for issues...\n",
            "âœ“ Missing values: 0\n",
            "âœ“ Infinite values: 0\n",
            "âœ“ All numeric: True\n",
            "âœ“ X shape: (590540, 163)\n",
            "âœ“ y shape: (590540,)\n",
            "âœ“ Shapes match: True\n",
            "\n",
            "âœ… Data quality verified - Ready for model training!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**SAVE ENGINEERED FEATURES**"
      ],
      "metadata": {
        "id": "tbFx_MZQYBnK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"SAVING ENGINEERED FEATURES\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Save features and target\n",
        "X.to_csv('data/processed/X_features.csv', index=False)\n",
        "y.to_csv('data/processed/y_target.csv', index=False, header=True)\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: data/processed/X_features.csv\")\n",
        "print(f\"   Shape: {X.shape}\")\n",
        "print(f\"   Size: {os.path.getsize('data/processed/X_features.csv') / 1024**2:.2f} MB\")\n",
        "\n",
        "print(f\"ðŸ’¾ Saved: data/processed/y_target.csv\")\n",
        "print(f\"   Shape: {y.shape}\")\n",
        "\n",
        "# Save TransactionID if exists\n",
        "if 'transaction_ids' in locals():\n",
        "    transaction_ids.to_csv('data/processed/transaction_ids.csv', index=False, header=True)\n",
        "    print(f\"ðŸ’¾ Saved: data/processed/transaction_ids.csv\")\n",
        "\n",
        "# Save final column info\n",
        "final_column_info = {\n",
        "    'feature_columns': X.columns.tolist(),\n",
        "    'target_column': 'isFraud',\n",
        "    'numeric_columns': [col for col in numeric_cols if col in X.columns],\n",
        "    'encoded_categorical_columns': list(label_encoders.keys()),\n",
        "    'new_features': new_features_created,\n",
        "    'removed_features': high_corr_features,\n",
        "    'total_features': X.shape[1]\n",
        "}\n",
        "\n",
        "with open('results/03_final_column_info.pkl', 'wb') as f:\n",
        "    pickle.dump(final_column_info, f)\n",
        "\n",
        "print(\"ðŸ’¾ Saved: results/03_final_column_info.pkl\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uciAQwwzYCgS",
        "outputId": "921ce35c-c4e6-477b-e60a-dc53dfbe5549"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "SAVING ENGINEERED FEATURES\n",
            "============================================================\n",
            "ðŸ’¾ Saved: data/processed/X_features.csv\n",
            "   Shape: (590540, 163)\n",
            "   Size: 373.41 MB\n",
            "ðŸ’¾ Saved: data/processed/y_target.csv\n",
            "   Shape: (590540,)\n",
            "ðŸ’¾ Saved: data/processed/transaction_ids.csv\n",
            "ðŸ’¾ Saved: results/03_final_column_info.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**FEATURE ENGINEERING SUMMARY**"
      ],
      "metadata": {
        "id": "3Iz3YepvYFfM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"FEATURE ENGINEERING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "summary = f\"\"\"\n",
        "FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
        "\n",
        "Input Shape: {train_df.shape}\n",
        "Output Shape: X={X.shape}, y={y.shape}\n",
        "\n",
        "Steps Completed:\n",
        "1. âœ“ Separated features and target variable\n",
        "2. âœ“ Created {len(new_features_created)} new features\n",
        "3. âœ“ Encoded {len(categorical_cols)} categorical variables using Label Encoding\n",
        "4. âœ“ Removed {len(high_corr_features)} highly correlated features (>0.95)\n",
        "5. âœ“ Verified data quality (no missing/infinite values)\n",
        "\n",
        "Final Dataset:\n",
        "- Total features: {X.shape[1]}\n",
        "- Numeric features: {len([col for col in numeric_cols if col in X.columns])}\n",
        "- Encoded categorical: {len(label_encoders)}\n",
        "- Target distribution: {dict(y.value_counts())}\n",
        "- All features are numeric: âœ“\n",
        "- Ready for scaling and modeling: âœ“\n",
        "\n",
        "Files Saved:\n",
        "- X_features.csv: Feature matrix\n",
        "- y_target.csv: Target variable\n",
        "- label_encoders.pkl: Encoders for test data\n",
        "- encoding_mappings.pkl: Encoding reference\n",
        "- final_column_info.pkl: Feature metadata\n",
        "\n",
        "Next Step: Proceed to Notebook 4 for Handling Imbalance & Scaling\n",
        "\"\"\"\n",
        "\n",
        "print(summary)\n",
        "\n",
        "# Save summary\n",
        "with open('results/03_feature_engineering_summary.txt', 'w') as f:\n",
        "    f.write(summary)\n",
        "\n",
        "print(\"ðŸ’¾ Saved: results/03_feature_engineering_summary.txt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YNue11w1YHk4",
        "outputId": "94dff857-64da-44d7-feb9-8e5b01a0f76f"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "FEATURE ENGINEERING SUMMARY\n",
            "============================================================\n",
            "\n",
            "FEATURE ENGINEERING COMPLETED SUCCESSFULLY!\n",
            "\n",
            "Input Shape: (590540, 219)\n",
            "Output Shape: X=(590540, 163), y=(590540,)\n",
            "\n",
            "Steps Completed:\n",
            "1. âœ“ Separated features and target variable\n",
            "2. âœ“ Created 0 new features\n",
            "3. âœ“ Encoded 9 categorical variables using Label Encoding\n",
            "4. âœ“ Removed 55 highly correlated features (>0.95)\n",
            "5. âœ“ Verified data quality (no missing/infinite values)\n",
            "\n",
            "Final Dataset:\n",
            "- Total features: 163\n",
            "- Numeric features: 154\n",
            "- Encoded categorical: 9\n",
            "- Target distribution: {0: np.int64(569877), 1: np.int64(20663)}\n",
            "- All features are numeric: âœ“\n",
            "- Ready for scaling and modeling: âœ“\n",
            "\n",
            "Files Saved:\n",
            "- X_features.csv: Feature matrix\n",
            "- y_target.csv: Target variable\n",
            "- label_encoders.pkl: Encoders for test data\n",
            "- encoding_mappings.pkl: Encoding reference\n",
            "- final_column_info.pkl: Feature metadata\n",
            "\n",
            "Next Step: Proceed to Notebook 4 for Handling Imbalance & Scaling\n",
            "\n",
            "ðŸ’¾ Saved: results/03_feature_engineering_summary.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MEMORY CLEANUP**"
      ],
      "metadata": {
        "id": "AmMMxxrhYKpO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"MEMORY CLEANUP\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "del train_df, column_info, correlation_matrix, upper_triangle\n",
        "gc.collect()\n",
        "\n",
        "print(\"âœ“ Memory cleaned\")\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"NOTEBOOK 3 COMPLETE!\")\n",
        "print(\"=\" * 60)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiWJ95QdYMhj",
        "outputId": "a414dc02-2cd9-4468-eb74-e678a49affdc"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "MEMORY CLEANUP\n",
            "============================================================\n",
            "âœ“ Memory cleaned\n",
            "\n",
            "============================================================\n",
            "NOTEBOOK 3 COMPLETE!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ]
}